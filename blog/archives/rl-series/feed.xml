<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RL Series on weeeblog</title><link>https://nikhilweee.me/blog/archives/rl-series/</link><description>Recent content in RL Series on weeeblog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>nikhilweee@gmail.com (Nikhil Verma)</managingEditor><webMaster>nikhilweee@gmail.com (Nikhil Verma)</webMaster><copyright>&amp;copy; Nikhil Verma</copyright><lastBuildDate>Mon, 06 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://nikhilweee.me/blog/archives/rl-series/feed.xml" rel="self" type="application/rss+xml"/><item><title>A Walkthrough of Isaac Gym</title><link>https://nikhilweee.me/blog/2022/isaac-gym-walkthrough/</link><pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate><author>nikhilweee@gmail.com (Nikhil Verma)</author><guid>https://nikhilweee.me/blog/2022/isaac-gym-walkthrough/</guid><description>Introduction Training a policy using deep reinforcement learning consists of an agent interacting with the environment in a continuous loop. In practise, the agent is often modelled by deep networks that can take advantage of GPU parallelization, but the environment is still modelled by simulators that rely on the CPU. While the poor sample efficiency of RL algorithms remains a huge bottleneck, a significant amount of time is also spent on moving tensors from the CPU to the GPU, not to forget the additional delays caused by the lack of parallelism in CPU based simulation.</description></item></channel></rss>