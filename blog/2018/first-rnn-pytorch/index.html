<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Building your first RNN - weeeblog</title><meta name=description content="If you have some understanding of recurrent networks, want to get your hands dirty, but haven’t really tried to do that on your own, then you are certainly at the right place. This tutorial is a practical guide about getting started with recurrent networks using PyTorch. We’ll solve a simple cipher using PyTorch 0.4.0, which is the latest version at the time of this writing.
You are only expected to have some understanding of recurrent networks."><link rel=icon type=image/svg href=/static/favicon.png><link rel=stylesheet href=/main.css><link rel=canonical href=https://nikhilweee.me/blog/2018/first-rnn-pytorch/><script type=text/x-mathjax-config>
            MathJax.Hub.Config({
                tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
                CommonHTML: { linebreaks: { automatic: true } },
            });
        </script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href=https://cdn.jsdelivr.net/npm/charter-webfont/charter.min.css rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css></head><body><header class="site-header flex-parent"><div class=sidebar></div><div class=wrapper><a class=site-title href=/blog>weeeblog</a><nav class=site-nav><a id=theme-toggle class=page-link><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a><a class=page-link href=/blog/archives/>Archives</a>
<a class=page-link href=/>About</a></nav></div><div class=sidebar></div></header><main class="page-content flex-parent"><div class=sidebar></div><div class=wrapper><article class=post itemscope itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class=post-title itemprop="name headline">Building your first RNN</h1><h2 class=post-subtitle itemprop="abstract description">with PyTorch 0.4!</h2><p class=post-meta><time datetime="2018-05-24 00:00:00 +0000 UTC" itemprop=datePublished>May 24, 2018</time>
• <span itemprop=author itemscope itemtype=http://schema.org/Person><a href=/><span itemprop=name>Nikhil Verma</span></a></span>
•
<a href=/blog/categories/tutorials/>Tutorials</a></p></header><div class=post-content itemprop=articleBody><p>If you have some understanding of recurrent networks, want to get your hands dirty, but haven&rsquo;t really tried to do that on your own, then you are certainly at the right place. This tutorial is a practical guide about getting started with recurrent networks using PyTorch. We&rsquo;ll solve a simple cipher using PyTorch 0.4.0, which is the latest version at the time of this writing.</p><p>You are only expected to have some understanding of recurrent networks. If you don&rsquo;t, here&rsquo;s the link to the <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>golden resource</a> - Chris Olah&rsquo;s post on Understanding LSTMs. We&rsquo;ll use a single layer LSTM for the task of learning ciphers, which should be a fairly easy exercise.</p><h2 id=the-problem>The Problem</h2><p>Before starting off, let&rsquo;s first define the problem in a concrete manner. We wish to decrypt secret messages using an LSTM. For the sake of simplicity, let&rsquo;s assume that our messages are encrypted using the <a href=https://en.wikipedia.org/wiki/Caesar_cipher>Caesar Cipher</a>, which is a really simple substitution cipher.</p><p>Caesar cipher works by replacing each letter of the original message by another letter from a given alphabet to form an encrypted message. In this tutorial we&rsquo;ll use a right shift of 13, which basically means that the encrypted version of each letter in the alphabet is the one which occurs 13 places to the right of it. So <code>A</code>(1) becomes <code>N</code>(1+13), <code>B</code>(2) becomes <code>O</code>(2+13), and so on. Our alphabet will only include uppercase English characters <code>A</code> through <code>Z</code>, and an extra letter, <code>-</code>, to represent any foreign character.</p><p>With all of these in mind, here&rsquo;s the substitution table for your reference.</p><pre tabindex=0><code>A B C D E F G H I J K L M N O P Q R S T U V W X Y Z -
N O P Q R S T U V W X Y Z - A B C D E F G H I J K L M
</code></pre><p>The first row shows all the letters of the alphabet in order. To encrypt a message, each letter of the first row can be substituted by the corresponding letter from the second row. As an example, the message <code>THIS-IS-A-SECRET</code> becomes <code>FUVEMVEMNMERPDRF</code> when encrypted.</p><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>Aside : but <a href=#fn:why-nn>why use neural networks for this problem?</a></p><h2 id=the-dataset>The Dataset</h2><p>Like any other neural network, we&rsquo;ll need data. Loads of it. We&rsquo;ll use a parallel dataset of the following form where each tuple represents a pair of (encrypted, decrypted) messages.</p><pre tabindex=0><code>(&#39;FUVEMVEMNMERPDRF&#39;, &#39;THIS-IS-A-SECRET&#39;)
(&#39;FUVEMVEMN-AFURDMERPDRF&#39;, &#39;THIS-IS-ANOTHER-SECRET&#39;)
...
</code></pre><p>Having defined our problem, we&rsquo;ll feed the <code>encrypted</code> message as the input to our LSTM and expect it to emit the original message as the target. Sounds simple right?</p><p>It does, except that we have a little problem. Neural networks are essentially number crunching machines, and have no idea how to hande our encrypted messages. We&rsquo;ll somehow have to convert our strings into numbers for the network to make sense of them.</p><h2 id=word-embeddings>Word Embeddings</h2><p>The way this is usually done is to use something called as word embeddings. The idea is to represent every character in the alphabet with its own $ D $ dimensional <strong>embedding vector</strong>, where $ D $ is usually called the embedding dimension. So let&rsquo;s say if we decide to use an <code>embedding_dim</code> of 5. This basically means that each of the 27 characters of the alphabet, <code>ABCDEFGHIJKLMNOPQRSTUVWXYZ-</code>, will have their own embedding vector of length 5.</p><p>Often, these vectors are stored together as $ V \times D $ dimensional <strong>embedding matrix</strong>, $ E $, where each row $ E[i] $ of the matrix represents the embedding vector for the character with index $ i $ in the alphabet. Here $ V $ is the length of the vocabulary (alphabet), which is 27 in our case. As an example, the whole embedding matrix $ E $ might look something like the one shown below.</p><pre tabindex=0><code>[[-1.4107, -0.8142,  0.8486,  2.8257, -0.7130],
 [ 0.5434,  3.8553,  2.9420, -2.8364, -4.0077], 
 [ 1.6781, -0.2496,  2.5569, -0.2952, -2.2911],
 ...
 [ 2.7912,  1.3261,  1.7603,  3.3852, -2.1643]]
</code></pre><p>$ E[0] $ then represents the word vector for <code>A</code>, which is <code>[-1.4107, -0.8142, 0.8486, 2.8257, -0.7130]</code>.</p><p><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>Aside : but <a href=#fn:char-embedding>I read something different!</a></p><p>P.S. I&rsquo;ll be using alphabet and vocabulary interchangably throughout this tutorial. Similarly, word embeddings, word vectors, character embeddings, or simply embeddings will mean the same thing.</p><h2 id=the-cipher>The Cipher</h2><p>Now that we have enough background, let&rsquo;s get our hands dirty and finally jump in to writing some code. The first thing we have to do is to create a dataset. And to do that, we first need to implement the cipher. Although we implement it as a simple function, it might be a good idea to implement the cipher as a class in the future.</p><script type=application/javascript src="https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e.js?file=module-cipher.py"></script><p>We create the <code>encode</code> function which uses the parameters <code>vocab</code> and <code>key</code> to encrypt each character. Since we&rsquo;re working with letters, <code>vocab</code> in this context simply means the alphabet. The encryption algorithm should be fairly easy to understand. Notice how we use the modulo operator in line <code>8</code> to prevent the indexes from overflowing.</p><p>To check the implementation, you can check for some random inputs. For example, ensure that <code>encrypt('ABCDEFGHIJKLMNOPQRSTUVWXYZ-')</code> returns <code>NOPQRSTUVWXYZ-ABCDEFGHIJKLM</code>.</p><h2 id=the-dataset-finally>The Dataset (Finally!)</h2><p>Okay, let&rsquo;s finally build the dataset. For the sake of simplicity, we&rsquo;ll use a random sequence of characters as a message and encrypt it to create the input to the LSTM. To implement this, we create a simple function called <code>dataset</code> which takes in the parameter <code>num_examples</code> and returns a list of those many (input, output) pairs.</p><script type=application/javascript src="https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e.js?file=module-batch.py"></script><p>There&rsquo;s something strange about this function though. Have a look at line 24. We&rsquo;re not returning a pair of strings. We&rsquo;re first converting strings into a list of indices which represent their position in the alphabet. If you recall the section on <a href=#word-embeddings>word embeddings</a>, these indices will later be used to extract the corresponding embedding vectors from the embedding matrix $ E $. We&rsquo;re then converting these lists into a pair of tensors, which is what the function returns.</p><h2 id=tensors>Tensors?</h2><p>This brings us to the most fundamental data type in PyTorch - the Tensor. For users familiar with NumPy, a tensor is the PyTorch analogue of <code>ndarray</code>. If you&rsquo;re not, a tensor is essentially a multidimensional matrix which supports optimized implementations of common operations. Have a look at the <a href=http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html>Tensor Tutorial</a> on the PyTorch website for more information. The takeaway here is that we&rsquo;ll use tensors from now on as our go to data structure to handle numbers. Creating a tensor is really easy. Though there are a lot of ways to do so, we&rsquo;ll just wrap our list of integers with <code>torch.tensor()</code> - which turns out the easiest amongst all.</p><p>You can satisfy yourself by having a look at what this function does. A quick call to <code>dataset(1)</code> should return something similar to the following. You can also verify that the numbers in the second tensor are right shifted by 13 from the numbers in the first tensor. <code>20 = (7 + 13) % 27</code>, <code>3 = (17 + 13) % 27</code> and so on.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>[[</span><span style=color:#111>tensor</span><span style=color:#111>([</span> <span style=color:#ae81ff>20</span><span style=color:#111>,</span>   <span style=color:#ae81ff>3</span><span style=color:#111>,</span>  <span style=color:#ae81ff>21</span><span style=color:#111>,</span>   <span style=color:#ae81ff>0</span><span style=color:#111>,</span>  <span style=color:#ae81ff>14</span><span style=color:#111>,</span>   <span style=color:#ae81ff>4</span><span style=color:#111>,</span>   <span style=color:#ae81ff>2</span><span style=color:#111>,</span>   <span style=color:#ae81ff>4</span><span style=color:#111>,</span>  <span style=color:#ae81ff>13</span><span style=color:#111>,</span>  <span style=color:#ae81ff>12</span><span style=color:#111>,</span>   <span style=color:#ae81ff>8</span><span style=color:#111>,</span>  <span style=color:#ae81ff>23</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>         <span style=color:#ae81ff>12</span><span style=color:#111>,</span>  <span style=color:#ae81ff>10</span><span style=color:#111>,</span>  <span style=color:#ae81ff>25</span><span style=color:#111>,</span>  <span style=color:#ae81ff>17</span><span style=color:#111>,</span>  <span style=color:#ae81ff>19</span><span style=color:#111>,</span>   <span style=color:#ae81ff>1</span><span style=color:#111>,</span>   <span style=color:#ae81ff>2</span><span style=color:#111>,</span>  <span style=color:#ae81ff>22</span><span style=color:#111>,</span>  <span style=color:#ae81ff>12</span><span style=color:#111>,</span>  <span style=color:#ae81ff>15</span><span style=color:#111>,</span>  <span style=color:#ae81ff>16</span><span style=color:#111>,</span>   <span style=color:#ae81ff>3</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>         <span style=color:#ae81ff>13</span><span style=color:#111>,</span>  <span style=color:#ae81ff>10</span><span style=color:#111>,</span>  <span style=color:#ae81ff>20</span><span style=color:#111>,</span>  <span style=color:#ae81ff>23</span><span style=color:#111>,</span>  <span style=color:#ae81ff>25</span><span style=color:#111>,</span>  <span style=color:#ae81ff>15</span><span style=color:#111>,</span>  <span style=color:#ae81ff>19</span><span style=color:#111>,</span>   <span style=color:#ae81ff>4</span><span style=color:#111>]),</span> 
</span></span><span style=display:flex><span>  <span style=color:#111>tensor</span><span style=color:#111>([</span>  <span style=color:#ae81ff>7</span><span style=color:#111>,</span>  <span style=color:#ae81ff>17</span><span style=color:#111>,</span>   <span style=color:#ae81ff>8</span><span style=color:#111>,</span>  <span style=color:#ae81ff>14</span><span style=color:#111>,</span>   <span style=color:#ae81ff>1</span><span style=color:#111>,</span>  <span style=color:#ae81ff>18</span><span style=color:#111>,</span>  <span style=color:#ae81ff>16</span><span style=color:#111>,</span>  <span style=color:#ae81ff>18</span><span style=color:#111>,</span>   <span style=color:#ae81ff>0</span><span style=color:#111>,</span>  <span style=color:#ae81ff>26</span><span style=color:#111>,</span>  <span style=color:#ae81ff>22</span><span style=color:#111>,</span>  <span style=color:#ae81ff>10</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>         <span style=color:#ae81ff>26</span><span style=color:#111>,</span>  <span style=color:#ae81ff>24</span><span style=color:#111>,</span>  <span style=color:#ae81ff>12</span><span style=color:#111>,</span>   <span style=color:#ae81ff>4</span><span style=color:#111>,</span>   <span style=color:#ae81ff>6</span><span style=color:#111>,</span>  <span style=color:#ae81ff>15</span><span style=color:#111>,</span>  <span style=color:#ae81ff>16</span><span style=color:#111>,</span>   <span style=color:#ae81ff>9</span><span style=color:#111>,</span>  <span style=color:#ae81ff>26</span><span style=color:#111>,</span>   <span style=color:#ae81ff>2</span><span style=color:#111>,</span>   <span style=color:#ae81ff>3</span><span style=color:#111>,</span>  <span style=color:#ae81ff>17</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span><span style=color:#111>,</span>  <span style=color:#ae81ff>24</span><span style=color:#111>,</span>   <span style=color:#ae81ff>7</span><span style=color:#111>,</span>  <span style=color:#ae81ff>10</span><span style=color:#111>,</span>  <span style=color:#ae81ff>12</span><span style=color:#111>,</span>   <span style=color:#ae81ff>2</span><span style=color:#111>,</span>   <span style=color:#ae81ff>6</span><span style=color:#111>,</span>  <span style=color:#ae81ff>18</span><span style=color:#111>])]]</span>
</span></span></code></pre></td></tr></table></div></div><p>With this we&rsquo;re done with the basics. Let&rsquo;s start building the network. It&rsquo;s a good idea to first have a general overview of what we aim to achieve. One might think of something along the following lines.</p><blockquote><p>On a very high level, the first step in a general workflow will be to feed in inputs to an LSTM to get the predictions. Next, we pass on the predictions along with the targets to the loss function to calculate the loss. Finally, we backpropagate through the loss to update our model&rsquo;s parameters.</p></blockquote><p>Hmm, that sounds easy, right? But how do you actually make it work? Let&rsquo;s dissect this step by step. We&rsquo;ll first identify the components needed to build our model, and finally put them to gether as a single piece to make it work.</p><h3 id=the-pytorch-paradigm>The PyTorch paradigm</h3><p>&mldr; before diving in, it&rsquo;s important to know a couple of things. PyTorch provides implementations for most of the commonly used entities from layers such as LSTMs, CNNs and GRUs to optimizers like SGD, Adam, and what not (Isn&rsquo;t that the whole point of using PyTorch in the first place?!). The general paradigm to use any of these entities is to first create an instance of <code>torch.nn.entity</code> with some required parameters. As an example, here&rsquo;s how we instantiate an <code>lstm</code>.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1</span>
</span></span><span style=display:flex><span><span style=color:#111>lstm</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LSTM</span><span style=color:#111>(</span><span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>5</span><span style=color:#111>,</span> <span style=color:#111>hidden_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>10</span><span style=color:#111>,</span> <span style=color:#111>batch_first</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Next, we call this object with the inputs as parameters when we actually want to run an LSTM over some inputs. This is shown in the third line below.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>lstm_in</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>40</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>hidden_in</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>zeros</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>,</span> <span style=color:#ae81ff>10</span><span style=color:#111>),</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>zeros</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>,</span> <span style=color:#ae81ff>10</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2</span>
</span></span><span style=display:flex><span><span style=color:#111>lstm_out</span><span style=color:#111>,</span> <span style=color:#111>lstm_hidden</span> <span style=color:#f92672>=</span> <span style=color:#111>lstm</span><span style=color:#111>(</span><span style=color:#111>lstm_in</span><span style=color:#111>,</span> <span style=color:#111>hidden_in</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><p>This two-stepped process will be seen all through this tutorial and elsewhere. Below, we&rsquo;ll go through step 1 of all the modules. We&rsquo;ll connect the dots at a later stage.</p><p>Getting back to code now, let&rsquo;s dissect our &lsquo;high level&rsquo; understanding again.</p><h2 id=1-prepare-inputs>1. Prepare inputs</h2><blockquote><p>&mldr; <strong>feed in inputs</strong> to an LSTM to get the predictions &mldr;</p></blockquote><p>To feed in inputs, well, we first need to prepare the inputs. Remember the embedding matrix $ E $ we described <a href=#the-dataset-finally>earlier</a>? we&rsquo;ll use $ E $ to convert the pair of indices we get from <code>dataset()</code> into the corresponding embedding vectors. Following the general paradigm, we create an instance of <code>torch.nn.Embedding</code>.</p><p>The <a href=https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding>docs</a> list two required parameters - <code>num_embeddings: the size of the dictionary of embeddings</code> and <code>embedding_dim: the size of each embedding vector</code>. In our case, these are <code>vocab_size</code> $ V $ and <code>embedding_dim</code> $ D $ respectively.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1</span>
</span></span><span style=display:flex><span><span style=color:#111>embed</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Embedding</span><span style=color:#111>(</span><span style=color:#111>vocab_size</span><span style=color:#111>,</span> <span style=color:#111>embedding_dim</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Later on, we could easily convert any input tensor <code>ecrypted</code> containing indices of the encrypted input (like the one we get from <code>dataset()</code>) into the corresponding embedding vectors by simply calling <code>embed(encrypted)</code>.</p><p>As an example, the word <code>SECRET</code> becomes <code>ERPDRF</code> after encryption, and the letters of <code>ERPDRF</code> correspond to the indices <code>[4, 17, 15, 3, 17, 5]</code>. If <code>encrypted</code> is <code>torch.tensor([4, 17, 15, 3, 17, 5])</code>, then <code>embed(encrypted)</code> would return something similar to the following.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#111>encrypted</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>4</span><span style=color:#111>,</span> <span style=color:#ae81ff>17</span><span style=color:#111>,</span> <span style=color:#ae81ff>15</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>17</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#111>embedded</span> <span style=color:#f92672>=</span> <span style=color:#111>embed</span><span style=color:#111>(</span><span style=color:#111>encrypted</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>embedded</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>tensor</span><span style=color:#111>([[</span> <span style=color:#ae81ff>0.2666</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.1146</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.3225</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.3261</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.6993</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>        <span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1.5723</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.1346</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.6892</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.7130</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.7636</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>        <span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1.9679</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8601</span><span style=color:#111>,</span>  <span style=color:#ae81ff>3.0942</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8810</span><span style=color:#111>,</span>  <span style=color:#ae81ff>0.6042</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>        <span style=color:#111>[</span> <span style=color:#ae81ff>3.6624</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3556</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.7088</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.4370</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>3.2903</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>        <span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1.5723</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.1346</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.6892</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.7130</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.7636</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>        <span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1.8041</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.8606</span><span style=color:#111>,</span>  <span style=color:#ae81ff>2.5406</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>3.5191</span><span style=color:#111>,</span>  <span style=color:#ae81ff>1.7761</span><span style=color:#111>]])</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=2-build-an-lstm>2. Build an LSTM</h2><blockquote><p>&mldr; feed in inputs <strong>to an LSTM</strong> to get the predictions &mldr;</p></blockquote><p>Next, we need to create an LSTM. We do this in a similar fashion by creating an instance of <code>torch.nn.LSTM</code>. This time, the <a href=https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM>docs</a> list the required parameters as <code>input_size: the number of expected features in the input</code> and <code>hidden_size: the number of features in the hidden state</code>. Since LSTMs typically operate on variable length sequences, the <code>input_size</code> refers to the size of each entity in the input sequence. In our case, this means the <code>embedding_dim</code>. This might sound counter-intuitive, but if you think for a while, it makes sense.</p><p><code>hidden_size</code>, as the name suggests, is the size of the hidden state of the RNN. In case of an LSTM, this refers to the size of both, the <code>cell_state</code> and the <code>hidden_state</code>. Note that the hidden size is a hyperparameter and <em>can be different</em> from the input size. <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>colah&rsquo;s blog post</a> doesn&rsquo;t explicitly mention this, but the equations on the PyTorch <a href=https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell>docs on LSTMCell</a> should make it clear. To summarize the discussion above, here is how we instantiate the LSTM.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1</span>
</span></span><span style=display:flex><span><span style=color:#111>lstm</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LSTM</span><span style=color:#111>(</span><span style=color:#111>embedding_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=a-note-on-dimensionality>A note on dimensionality</h3><p>During step 2 of the <a href=#the-pytorch-paradigm>general paradigm</a>, <code>torch.nn.LSTM</code> expects the input to be a 3D input tensor of size <code>(seq_len, batch, embedding_dim)</code>, and returns an output tensor of the size <code>(seq_len, batch, hidden_dim)</code>. We&rsquo;ll only feed in one input at a time, so <code>batch</code> is always <code>1</code>.</p><p>As an example, consider the input-output pair <code>('ERPDRF', 'SECRET')</code>. Using an <code>embedding_dim</code> of 5, the 6 letter long input <code>ERPDRF</code> is transformed into an input tensor of size <code>6 x 1 x 5</code>. If <code>hidden_dim</code> is 10, the input is processed by the LSTM into an output tensor of size <code>6 x 1 x 10</code>.</p><p>Generally, the LSTM is expected to run over the input sequence character by character to emit a probability distribution over all the letters in the vocabulary. So for every input character, we expect a $ V $ dimensional output tensor where $ V $ is 27 (the size of the vocabulary). The most probable letter is then chosen as the output at every timestep.</p><p>If you have a look at the output of the LSTM on the example pair <code>('ERPDRF', 'SECRET')</code> <a href=#a-note-on-dimensionality>above</a>, you can instantly make out that the dimensions are not right. The output dimension is <code>6 x 1 x 10</code> - which means that for every character, the output is a $ D $ (10) dimensional tensor instead of the expected 27.</p><p>So how do we solve this?</p><h2 id=3-transform-the-outputs>3. Transform the outputs</h2><blockquote><p>&mldr; feed in inputs to an LSTM to <strong>get the predictions</strong> &mldr;</p></blockquote><p>The general workaround is to transform the $ D $ dimensional tensor into a $ V $ dimensional tensor through what is called an affine (or linear) transform. Sparing the definitions aside, the idea is to use matrix multiplication to get the desired dimensions.</p><p>Let&rsquo;s say the LSTM produces an output tensor $ O $ of size <code>seq_len x batch x hidden_dim</code>. Recall that we only feed in one example at a time, so <code>batch</code> is always <code>1</code>. This essentially gives us an output tensor $ O $ of size <code>seq_len x hidden_dim</code>. Now if we multiply this output tensor with another tensor $ W $ of size <code>hidden_dim x embedding_dim</code>, the resultant tensor $ R = O \times W $ has a size of <code>seq_len x embedding_dim</code>. Isn&rsquo;t this exactly what we wanted?</p><p>To implement the linear layer, &mldr; you guessed it! We create an instance of <code>torch.nn.Linear</code>. This time, the <a href=https://pytorch.org/docs/stable/nn.html#torch.nn.Linear>docs</a> list the required parameters as <code>in_features: size of each input sample</code> and <code>out_features: size of each output sample</code>. Note that this only transforms the last dimension of the input tensor. So for example, if we pass in an input tensor of size <code>(d1, d2, d3, ..., dn, in_features)</code>, the output tensor will have the same size for all but the last dimension, and will be a tensor of size <code>(d1, d2, d3, ..., dn, out_features)</code>.</p><p>With this knowledge in mind, it&rsquo;s easy to figure out that <code>in_features</code> is <code>hidden_dim</code>, and <code>out_features</code> is <code>vocab_size</code>. The linear layer is initialised below.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1</span>
</span></span><span style=display:flex><span><span style=color:#111>linear</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><p>With this we&rsquo;re preddy much done with the essentials. Time for some learning!</p><h2 id=4-calculate-the-loss>4. Calculate the loss</h2><blockquote><p>Next, we pass on the predictions along with the targets to the loss function to calculate the loss.</p></blockquote><p>If you think about it, the LSTM is essentially performing multi-class classification at every time step by choosing one letter out of the 27 characters of the vocabulary. A common choice in such a case is to use the cross entropy loss function <code>torch.nn.CrossEntropyLoss</code>. We initialize this in a similar manner.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss_fn</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>CrossEntropyLoss</span><span style=color:#111>()</span>
</span></span></code></pre></td></tr></table></div></div><p>You can read more about cross entropy loss in the excellent <a href=https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/>blog post by Rob DiPietro.</a></p><h2 id=5-optimize>5. Optimize</h2><blockquote><p>Finally, we backpropagate through the loss to update our model’s parameters.</p></blockquote><p>A popular choice is the Adam optimizer. Here&rsquo;s how we initialize it. Notice that almost all torch layers have this convenient way of getting all their parameters by calling <code>module.parameters()</code>.</p><div class=highlight><div style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optimizer</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>Adam</span><span style=color:#111>(</span><span style=color:#111>list</span><span style=color:#111>(</span><span style=color:#111>embed</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>())</span> <span style=color:#f92672>+</span> <span style=color:#111>list</span><span style=color:#111>(</span><span style=color:#111>lstm</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span>                             <span style=color:#f92672>+</span> <span style=color:#111>list</span><span style=color:#111>(</span><span style=color:#111>linear</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>()),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span><span style=color:#111>)</span>
</span></span></code></pre></td></tr></table></div></div><p>To summarize, here&rsquo;s how we initialize the required layers.</p><script type=application/javascript src="https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e.js?file=module-model.py"></script><p>Let&rsquo;s wrap this up and consolidate the network. Have a look at the training script below. Most of the code should make sense on its own. There are a few helper operations like <code>torch.squeeze</code> and <code>torch.transpose</code> whose function can be inferred from the comments. You can also refer to the <a href=https://pytorch.org/docs/stable/torch.html>docs</a> for more information.</p><script type=application/javascript src="https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e.js?file=module-train.py"></script><p>After every training iteration, we need to evaluate the network. Have a look at the validation script below. After calculating the scores as in the training script, we calculate a softmax over the scores to get a probability distribution in line 9. We then aggregate the characters with the maximum probability in line 11. We then compare the predicted output <code>batch_out</code> with the target output <code>original</code> in line 15. At the end of the epoch, we calculate the accuracy in line 18.</p><script type=application/javascript src="https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e.js?file=module-valid.py"></script><p>Notice that the predicted outputs are still in the form of indices. Converting them back to characters is left as an exercise.</p><p>But before you go, congratulations! You&rsquo;ve built your first RNN in PyTorch! The complete code for this post is available as a <a href=https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e>GitHub gist</a>. You can test the network by simply running the <a href=https://gist.github.com/nikhilweee/13243631f8ed219167ccd3866ce3204e#file-train-py>training script</a>. Thanks for sticking around.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><strong>But why Neural Networks?</strong> You might be wondering why do we use neural networks in the first place. In our use case, it sure makes more sense to decrypt the messages by conventional programming because we <em>know</em> the encryption function beforehand. <em>This might not be the case everytime</em>. You might have a situation where you have enough data but still have no idea about the encryption function. Neural networks fit quite well in such a situation. Anyways, keep in mind that this is still a toy problem. One motivation to choose this problem is the ease of generating loads of training examples on the fly. So we don&rsquo;t really need to procure any dataset. Yay!&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><strong>I think I read something different!</strong> Strictly speaking, what I just described here is called a <em>character embedding</em>, beacause we have a vector for each <em>character</em> in the alphabet. In case we had a vector for each <em>word</em> in a vocabulary, we would be using <em>word embeddings</em> instead. Notice the analogy here. An alphabet is the set of all the letters in a language. Similarly, a vocabulary is the collection of all the words in a language.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//nikhilweee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></div><div class=sidebar><aside class=sidebar-toc><nav id=TableOfContents><ul><li><ul><li><a href=#the-problem>The Problem</a></li><li><a href=#the-dataset>The Dataset</a></li><li><a href=#word-embeddings>Word Embeddings</a></li><li><a href=#the-cipher>The Cipher</a></li><li><a href=#the-dataset-finally>The Dataset (Finally!)</a></li><li><a href=#tensors>Tensors?</a><ul><li><a href=#the-pytorch-paradigm>The PyTorch paradigm</a></li></ul></li><li><a href=#1-prepare-inputs>1. Prepare inputs</a></li><li><a href=#2-build-an-lstm>2. Build an LSTM</a><ul><li><a href=#a-note-on-dimensionality>A note on dimensionality</a></li></ul></li><li><a href=#3-transform-the-outputs>3. Transform the outputs</a></li><li><a href=#4-calculate-the-loss>4. Calculate the loss</a></li><li><a href=#5-optimize>5. Optimize</a></li></ul></li></ul></nav></aside></div></main><footer class="site-footer flex-parent"><div class=sidebar></div><div class=wrapper><p>&copy; 2022 <a href=/>Nikhil Verma</a> • Subscribe via <a href=/feed.xml>RSS</a></p></div><div class=sidebar></div></footer></body><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark"),document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></html>