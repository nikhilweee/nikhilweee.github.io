<!doctype html><!-- begin: layouts/_default/baseof.html --><html lang=en><!-- begin: layouts/partials/head.html --><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.101.0"><title>Speed up your remote workflow - weeeblog</title><link rel=icon type=image/svg href=/static/favicon.png><link rel=stylesheet type=text/css href=/main.css><link rel=canonical href=https://nikhilweee.me/blog/2022/speed-up-remote-workflow-slurm-nyu-hpc/><link href=https://cdn.jsdelivr.net/npm/charter-webfont/charter.min.css rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css></head><!-- end: layouts/partials/head.html --><body class=flex-parent><div class="sidebar sidebar-left"></div><main class=flex-content><!-- begin: layouts/partials/header.html --><div class=wrapper><header class=site-header><nav class=site-title><a href=/blog><strong>weeeblog</strong></a></nav><nav class=site-nav><a href=/>About</a>
<a href=/blog/>Blog</a>
<a href=/blog/archives>Archives</a></nav><nav class=site-toggle><a id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></nav></header></div><!-- end: layouts/partials/header.html --><div class="page-content wrapper"><!-- begin: layouts/_default/single.html --><article class=post itemscope itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class=post-title itemprop="name headline">Speed up your remote workflow</h1><p class=post-subtitle itemprop="abstract description">using SLURM on NYU's HPC cluster</p><p class=post-meta><span itemprop=author itemscope itemtype=http://schema.org/Person><a href=/><span itemprop=name>Nikhil Verma</span></a></span> •
<time datetime="2022-02-13 00:00:00 +0000 UTC" itemprop=datePublished>Published Feb 13, 2022</time> •
<time datetime="2022-02-13 00:00:00 +0000 UTC" itemprop=datePublished>Updated Jul 25, 2022</time> •
<a href=/blog/archives/references/>References</a></p></header><div class=post-content itemprop=articleBody><p>Although NYU already a dedicated <a href=https://sites.google.com/nyu.edu/nyu-hpc>website</a> with all the documentation to get started with the HPC cluster, here&rsquo;s a set of tricks that I use to ease up the workflow by a significant amount. Even if you&rsquo;re not affiliated with NYU, I believe you&rsquo;d still be able to benefit from this post if your organization uses <a href=https://en.wikipedia.org/wiki/Slurm_Workload_Manager>Slurm</a>.</p><div class=post-toc><h3>Outline</h3><nav id=TableOfContents><ul><li><a href=#getting-started>Getting Started</a><ul><li><a href=#1-passwordless-ssh>1. Passwordless SSH</a></li><li><a href=#2-set-up-ssh-aliases>2. Set up SSH aliases</a></li></ul></li><li><a href=#login-hacks>Login Hacks</a><ul><li><a href=#1-login-directly-into-compute-nodes>1. Login directly into compute nodes</a></li><li><a href=#2-use-local-ssh-keys>2. Use local SSH keys</a></li></ul></li><li><a href=#some-slurm-aliases>Some Slurm Aliases</a><ul><li><a href=#1-common-aliases>1. Common aliases</a></li><li><a href=#2-view-your-job-queue>2. View your job queue</a></li><li><a href=#3-view-output-from-your-job>3. View output from your job</a></li></ul></li><li><a href=#some-hidden-gems>Some Hidden Gems</a></li><li><a href=#burst-usage>Burst Usage</a><ul><li><a href=#1-housekeeping>1. Housekeeping</a></li><li><a href=#2-compute-nodes>2. Compute Nodes</a></li><li><a href=#3-data-transfer>3. Data Transfer</a></li><li><a href=#4-gpu-nodes>4. GPU Nodes</a></li><li><a href=#5-port-forwarding>5. Port Forwarding</a></li></ul></li><li><a href=#troubleshooting>Troubleshooting</a><ul><li><a href=#1-singularity-cant-open-overlay-for-writing>1. Singularity can&rsquo;t open overlay for writing</a></li><li><a href=#2-job-killed-due-of-low-gpu-usage>2. Job killed due of low GPU usage</a></li></ul></li></ul></nav></div><h1 id=getting-started>Getting Started</h1><hr><p>Here&rsquo;s something that I recommend doing for any remote server that you&rsquo;ll access frequently.</p><h2 id=1-passwordless-ssh>1. Passwordless SSH</h2><p>If you haven&rsquo;t done so yet, you should definitely add your ssh keys to the cluster to avoid typing in your password whenever you want to login. Replace <code>abc1234</code> with your NetID.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> ssh-copy-id abc1234@greene.hpc.nyu.edu
</span></span><span class=line><span class=cl><span class=go>...
</span></span></span><span class=line><span class=cl><span class=go></span><span class=gp>$</span> ssh-copy-id abc1234@access.cims.nyu.edu
</span></span><span class=line><span class=cl><span class=go>...
</span></span></span></code></pre></div><h2 id=2-set-up-ssh-aliases>2. Set up SSH aliases</h2><p>Keying in <code>ssh abc1234@greene.hpc.nyu.edu</code> or <code>ssh abc1234@access.cims.nyu.edu</code> is so long and so old school when you can just get away with something like <code>ssh greene</code> or <code>ssh cims</code>. You should definitely set up an alias instead. Edit <code>~/.ssh/config</code> in your favourite text editor:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.ssh/config</span>
</span></span><span class=line><span class=cl><span class=c1># NYU Greene</span>
</span></span><span class=line><span class=cl>Host greene
</span></span><span class=line><span class=cl>    Hostname greene.hpc.nyu.edu
</span></span><span class=line><span class=cl>    User abc1234
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># NYU CIMS</span>
</span></span><span class=line><span class=cl>Host cims
</span></span><span class=line><span class=cl>    Hostname access.cims.nyu.edu
</span></span><span class=line><span class=cl>    User abc1234
</span></span></code></pre></td></tr></table></div></div><h1 id=login-hacks>Login Hacks</h1><hr><p>These are a few hacks which should greatly simplify the overhead of using the cluster.</p><h2 id=1-login-directly-into-compute-nodes>1. Login directly into compute nodes</h2><p>Often times, you wish you could directly access the compute nodes (such as <code>linserv1</code>) without having to first SSH into <code>access.cims.nyu.edu</code>.
Here&rsquo;s how you can use the <code>ProxyJump</code> directive.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=hl><span class=lnt>5
</span></span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.ssh/config</span>
</span></span><span class=line><span class=cl><span class=c1># NYU CIMS</span>
</span></span><span class=line><span class=cl>Host access1 access2 linserv1
</span></span><span class=line><span class=cl>    User abc1234
</span></span><span class="line hl"><span class=cl>    ProxyJump cims
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Host cims
</span></span><span class=line><span class=cl>    Hostname access.cims.nyu.edu
</span></span><span class=line><span class=cl>    User abc1234
</span></span></code></pre></td></tr></table></div></div><p>Lines 7-9 describe the <code>login</code> node, and lines 3-5 describe the <code>compute</code> nodes. So now whenever you type <code>ssh linserv1</code>, you will first login to <code>cims</code> (<code>access.cims.nyu.edu</code>), and upon successful authentication, you will automatically SSH into <code>linserv1</code>. You can setup Greene in the same way.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=hl><span class=lnt>5
</span></span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.ssh/config</span>
</span></span><span class=line><span class=cl><span class=c1># NYU Greene</span>
</span></span><span class=line><span class=cl>Host log-1 log-2 log-3 log-4 log-5
</span></span><span class=line><span class=cl>    User abc1234
</span></span><span class="line hl"><span class=cl>    ProxyJump greene
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Host greene
</span></span><span class=line><span class=cl>    Hostname greene.hpc.nyu.edu
</span></span><span class=line><span class=cl>    User abc1234
</span></span></code></pre></td></tr></table></div></div><p>This setup is particularly helpful because whenever you SSH into <code>greene.hpc.nyu.edu</code>, you are randomly taken to one of the login nodes. Now let&rsquo;s say if you had <code>tmux</code> running on <code>log-3</code> and you lose connectivity. The next time you login, you might be taken into <code>log-4</code>. You type <code>tmux ls</code> and it complains about no open sessions. You&rsquo;re flabbergasted because you clearly remember running <code>tmux</code> a moment ago. Sadly, you didn&rsquo;t remember the hostname.</p><p>With this setup, you can try making a habit of always logging in to one particular node, say <code>ssh log-3</code>, and now you don&rsquo;t have to worry about remembering hostnames anymore.</p><h2 id=2-use-local-ssh-keys>2. Use local SSH keys</h2><p>Did you ever get annoyed when you tried to use <code>git clone</code> on one of the private repositories and you suddenly notice GitHub throwing an error? You may have added your local SSH keys to GitHub to for secure authentication but as you can imagine, GitHub does not recognise the keys from the cluster. The way around it is to use the <code>ForwardAgent</code> directive.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=hl><span class=lnt> 5
</span></span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=hl><span class=lnt>11
</span></span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.ssh/config</span>
</span></span><span class=line><span class=cl><span class=c1># NYU CIMS</span>
</span></span><span class=line><span class=cl>Host access1 access2 linserv1
</span></span><span class=line><span class=cl>    User abc1234
</span></span><span class="line hl"><span class=cl>    ForwardAgent yes
</span></span><span class=line><span class=cl>    ProxyJump cims
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Host cims
</span></span><span class=line><span class=cl>    Hostname access.cims.nyu.edu
</span></span><span class=line><span class=cl>    User abc1234
</span></span><span class="line hl"><span class=cl>    ForwardAgent yes
</span></span></code></pre></td></tr></table></div></div><p>This way, you can <em>forward</em> your local SSH keys to the cluster, and GitHub (or any other service) will receive your local keys so they&rsquo;ll stop complaining anymore. Just one more step before this works: make sure that your local keys are added to <code>ssh-agent</code>. Use the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># check if ssh-agent has keys loaded</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh-add -l
</span></span><span class=line><span class=cl><span class=go>The agent has no identities.
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># add local keys to ssh-agent</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh-add
</span></span><span class=line><span class=cl><span class=go>Identity added: /Users/nikhil/.ssh/id_rsa (nikhil-macbook)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># verify that keys have been successfully added</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh-add -l
</span></span><span class=line><span class=cl><span class=go>3072 SHA256:abcdefghijklmnopqrstuvwxyz nikhil-macbook (RSA)
</span></span></span></code></pre></div><h1 id=some-slurm-aliases>Some Slurm Aliases</h1><hr><p>If you&rsquo;re using Greene on a regular basis, chances are that you will need to use a few of the SLURM commands to check and manage your jobs. Here&rsquo;s a few aliases that I created, they should be handy for you too. Once logged into the cluster, add the following lines to <code>~/.bash_aliases</code>. Make sure to <code>source ~/.bashrc</code> every time you make a change for the aliases to take effect.</p><h2 id=1-common-aliases>1. Common aliases</h2><p>I often run an interactive job while debugging code. Here are some shortcuts that I use.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.bash_aliases</span>
</span></span><span class=line><span class=cl><span class=c1># Slurm Job Aliases</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># cpu job for 6 hours</span>
</span></span><span class=line><span class=cl><span class=nb>alias</span> <span class=nv>srunc6h</span><span class=o>=</span><span class=s1>&#39;srun --mem=64GB --time=6:00:00 --pty /bin/bash&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># gpu job for 1 hour</span>
</span></span><span class=line><span class=cl><span class=nb>alias</span> <span class=nv>srung1h</span><span class=o>=</span><span class=s1>&#39;srun --gres=gpu:1 --mem=16GB --time=1:00:00 --pty /bin/bash&#39;</span>
</span></span></code></pre></div><h2 id=2-view-your-job-queue>2. View your job queue</h2><p>These aliases help you view your job queue, albeit in a slightly improved format.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.bash_aliases</span>
</span></span><span class=line><span class=cl><span class=c1># Slurm Control Aliases</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># shows status of active jobs</span>
</span></span><span class=line><span class=cl><span class=nb>alias</span> <span class=nv>sq</span><span class=o>=</span><span class=s1>&#39;squeue -u abc1234 -o &#34;%.15i %.9N %.9T %.9M %.12L %.20V %j&#34; -S &#34;V&#34;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># run the previous command in a loop, every 1 sec</span>
</span></span><span class=line><span class=cl><span class=nb>alias</span> <span class=nv>watchsq</span><span class=o>=</span><span class=s1>&#39;watch -n 1 \
</span></span></span><span class=line><span class=cl><span class=s1>    squeue -u abc1234 -o \&#34;%.15i %.9N %.9T %.9M %.12L %.20V %j\&#34; -S &#34;V&#34;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># also shows completed jobs in the last 24 hours</span>
</span></span><span class=line><span class=cl><span class=nb>alias</span> <span class=nv>shist</span><span class=o>=</span><span class=s1>&#39;sacct -X -o &#34;JobID%15,ExitCode,State,Reason,\
</span></span></span><span class=line><span class=cl><span class=s1>    Submit,Elapsed,MaxVMSize,NodeList,Priority,JobName%20&#34;&#39;</span>
</span></span></code></pre></div><p>Here&rsquo;s an example. The best part about <code>sq</code> is that it shows the submit time, time elapsed, and time remaining so you can clearly identify your jobs. When you want to see completed jobs as well, use <code>shist</code>. Additional options can follow. For example, to filter jobs active between 1-10 February 2022, use <code>shist --starttime=2022-02-01 --endtime=2022-02-10</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> sq
</span></span><span class=line><span class=cl><span class=go>          JOBID  NODELIST     STATE      TIME    TIME_LEFT          SUBMIT_TIME NAME
</span></span></span><span class=line><span class=cl><span class=go>       14878566     gv002   RUNNING     22:13        37:47  2022-02-13T03:34:31 bash
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> shist
</span></span><span class=line><span class=cl><span class=go>          JobID ExitCode      State                 Reason              Submit    Elapsed  MaxVMSize        NodeList   Priority              JobName 
</span></span></span><span class=line><span class=cl><span class=go>--------------- -------- ---------- ---------------------- ------------------- ---------- ---------- --------------- ---------- -------------------- 
</span></span></span><span class=line><span class=cl><span class=go>       14878566      0:0    RUNNING                   None 2022-02-13T03:34:31   00:21:25                      gv002      18632                 bash 
</span></span></span><span class=line><span class=cl><span class=go>       14879278      0:0  COMPLETED                   None 2022-02-13T03:50:16   00:00:00                      cs008      17618              test.sh
</span></span></span></code></pre></div><h2 id=3-view-output-from-your-job>3. View output from your job</h2><p>These functions let you conveniently see the outputs for a given JOBID. For some reason they only work if the job is still running, so unfortunately you can&rsquo;t use them for completed jobs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ~/.bash_aliases</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Slurm Log Aliases</span>
</span></span><span class=line><span class=cl><span class=c1># slog &lt;JOBID&gt; shows location of the log file for a given job</span>
</span></span><span class=line><span class=cl><span class=k>function</span> slog<span class=o>()</span> <span class=o>{</span> scontrol show job <span class=s2>&#34;</span><span class=si>${</span><span class=nv>1</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> grep StdOut <span class=p>|</span> cut -d <span class=s2>&#34;=&#34;</span> -f <span class=m>2</span> <span class=p>;</span> <span class=o>}</span> 
</span></span><span class=line><span class=cl><span class=c1># catslog &lt;JOBID&gt; is just `cat` on the file location returned by slog</span>
</span></span><span class=line><span class=cl><span class=k>function</span> catslog<span class=o>()</span> <span class=o>{</span> cat <span class=k>$(</span>slog <span class=s2>&#34;</span><span class=si>${</span><span class=nv>1</span><span class=si>}</span><span class=s2>&#34;</span><span class=k>)</span> <span class=p>;</span> <span class=o>}</span>
</span></span><span class=line><span class=cl><span class=c1># catslog &lt;JOBID&gt; is just `tail -f` on the file location returned by slog</span>
</span></span><span class=line><span class=cl><span class=k>function</span> tailslog<span class=o>()</span> <span class=o>{</span> tail -f -n +1 <span class=k>$(</span>slog <span class=s2>&#34;</span><span class=si>${</span><span class=nv>1</span><span class=si>}</span><span class=s2>&#34;</span><span class=k>)</span> <span class=p>;</span> <span class=o>}</span>
</span></span></code></pre></div><p>Here are some examples. My general workflow is to use <code>sq</code> followed by <code>tailslog</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># path to the log file for 14879246</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> slog <span class=m>14879246</span>
</span></span><span class=line><span class=cl><span class=go>/home/abc1234/projects/sbatch/slurm-14879246.out
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># contents of /home/abc1234/projects/sbatch/slurm-14879246.out</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> tailslog <span class=m>14879278</span>
</span></span><span class=line><span class=cl><span class=go>archive  boot  etc   home  lib64  misc  net  proc  run   scratch  srv    sys  usr  vast
</span></span></span><span class=line><span class=cl><span class=go>bin      dev   gpfs  lib   media  mnt   opt  root  sbin  share    state  tmp  var
</span></span></span></code></pre></div><h1 id=some-hidden-gems>Some Hidden Gems</h1><hr><p>Here are some gems that I discovered during the course of my SLURM usage.</p><ul><li><code>seff &lt;jobid></code>: Displays CPU and Memory usage stats for the job.</li><li><code>sattach &lt;jobid.step></code>: Attaches to the console of an existing job.<br>This can be used as an alternative to the <a href=#3-view-output-from-your-job><code>tailslog()</code></a> function described above. If you don&rsquo;t know the <code>step</code> for a job, try using <code>0</code>. Please be aware that this doesn&rsquo;t always work.</li><li><code>sprio -u $USER</code>: Lists the scheduling priorities for pending jobs.</li></ul><h1 id=burst-usage>Burst Usage</h1><hr><p>For some courses, you might be alloted a specific number of GPU hours on GCP. To use these resources, you first need to login to the burst node from Greene. Next, you should check if you have access to a class account. If you do, you can specify that account to first submit a non-GPU job to setup your environment and code. When you&rsquo;re comfortable, you can submit a GPU job.</p><h2 id=1-housekeeping>1. Housekeeping</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Run these commands from greene</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># SSH into the burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh burst
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># Check your associated accounts and resource quotas</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sacctmgr show assoc <span class=nv>user</span><span class=o>=</span><span class=nv>$USER</span> <span class=nv>format</span><span class=o>=</span>Account%25,GrpTRESMins%30
</span></span><span class=line><span class=cl><span class=go>                  Account                    GrpTRESMins 
</span></span></span><span class=line><span class=cl><span class=go>------------------------- ------------------------------ 
</span></span></span><span class=line><span class=cl><span class=go>      csci-ga-2565-2022sp      cpu=120000,gres/gpu=12000 
</span></span></span><span class=line><span class=cl><span class=go>                    users 
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># Check your current usage</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sshare --user<span class=o>=</span><span class=nv>$USER</span>  --format<span class=o>=</span>Account,GrpTRESRaw%100
</span></span><span class=line><span class=cl><span class=go>             Account                                                                                 GrpTRESRaw 
</span></span></span><span class=line><span class=cl><span class=go>--------------------                                      ----------------------------------------------------- 
</span></span></span><span class=line><span class=cl><span class=go>csci-ga-2565-2022sp         cpu=110,mem=116000,energy=0,node=50,billing=110,fs/disk=0,vmem=0,pages=0,gres/gpu=1 
</span></span></span><span class=line><span class=cl><span class=go>users                          cpu=92,mem=69625,energy=0,node=46,billing=92,fs/disk=0,vmem=0,pages=0,gres/gpu=0 
</span></span></span></code></pre></div><h2 id=2-compute-nodes>2. Compute Nodes</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Now, submit a job using one of these accounts</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># This is not a GPU job and most likely should not count towards your quota</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sbatch --account<span class=o>=</span>csci-ga-2565-2022sp --partition<span class=o>=</span>interactive --time<span class=o>=</span>01:00:00 --wrap <span class=s2>&#34;sleep infinity&#34;</span>
</span></span><span class=line><span class=cl><span class=go>Submitted batch job 74566
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># Check the compute node where the job gets allocated</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> squeue -u <span class=nv>$USER</span> -O <span class=s2>&#34;JobID:10,Partition:15,Nodelist:10,State:15,StartTime,TimeUsed:10,TimeLeft:10,Account:25,Name&#34;</span>
</span></span><span class=line><span class=cl><span class=go>JOBID     PARTITION      NODELIST  STATE          START_TIME          TIME      TIME_LEFT ACCOUNT                  NAME                
</span></span></span><span class=line><span class=cl><span class=go>74566     interactive    b-9-90    RUNNING        2022-05-01T19:22:53 1:41      58:19     csci-ga-2565-2022sp      wrap
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># SSH into the compute node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh b-9-90
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Alternatively, you can also submit an interactive job instead of a batch job</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># If you do not specify the time, the job will terminate after one hour</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> srun --account<span class=o>=</span>csci-ga-2565-2022sp --partition<span class=o>=</span>interactive --pty /bin/bash
</span></span></code></pre></div><h2 id=3-data-transfer>3. Data Transfer</h2><p>Once you have spun up a compute node, you might be looking to transfer data from Greene. Keep in mind that Greene and the Burst login node share data, however the Burst compute nodes have their own persistent storage. This means that you can access your files that are stored on Greene from the burst node, but the compute nodes cannot access that data.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Run these commands from a compute node on burst</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Copy whatever files you need</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> scp <span class=nv>$USER</span>@greene-dtn.hpc.nyu.edu:/source/path /dest/path
</span></span></code></pre></div><h2 id=4-gpu-nodes>4. GPU Nodes</h2><p>You can use GPU nodes in an interactive fashion just like you spawned a compute node.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Submit a GPU Job. Notice we changed the arguments --partition and --gres</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sbatch --account<span class=o>=</span>csci-ga-2565-2022sp --partition<span class=o>=</span>n1s8-v100-1 --gres<span class=o>=</span>gpu --time<span class=o>=</span>01:00:00 --wrap <span class=s2>&#34;sleep infinity&#34;</span>
</span></span><span class=line><span class=cl><span class=go>Submitted batch job 74601
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># As before, see where this job gets allocated</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> squeue -u <span class=nv>$USER</span> -O <span class=s2>&#34;JobID:10,Partition:15,Nodelist:10,State:15,StartTime,TimeUsed:10,TimeLeft:10,Account:25,Name&#34;</span>
</span></span><span class=line><span class=cl><span class=go>JOBID     PARTITION      NODELIST  STATE          START_TIME          TIME      TIME_LEFT ACCOUNT                  NAME                
</span></span></span><span class=line><span class=cl><span class=go>74601     n1s8-v100-1    b-3-114   RUNNING        2022-05-01T19:54:23 1:01      58:59     csci-ga-2565-2022sp      wrap
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># SSH into the compute node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh b-3-114
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> <span class=c1># Now run whatever job you wish to run</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Enjoy!</span>
</span></span></code></pre></div><p>Alternatively, you can also use a script that you can submit using <code>sbatch</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --time=00-12:00:00</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --cpus-per-task=1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --gres=gpu:1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --output=&#34;sbatch_logs/%A_%x.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --account=csci-ga-2565-2022sp</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --partition=n1s8-v100-1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --job-name=pretrain_both</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>singularity <span class=nb>exec</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --nv --overlay /scratch/abc1234/overlay-50G-10M.ext3:ro <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    /scratch/abc1234/cuda11.3.0-cudnn8-devel-ubuntu20.04.sif <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    /bin/bash -c <span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    source /ext3/miniconda3/etc/profile.d/conda.sh;
</span></span></span><span class=line><span class=cl><span class=s2>    conda activate env;
</span></span></span><span class=line><span class=cl><span class=s2>    cd /home/abc1234/projects/project;
</span></span></span><span class=line><span class=cl><span class=s2>    python -u train.py --batch_size 40 \
</span></span></span><span class=line><span class=cl><span class=s2>        --output_dir runs/</span><span class=si>${</span><span class=nv>SLURM_JOB_NAME</span><span class=si>}</span><span class=s2>_</span><span class=si>${</span><span class=nv>SLURM_JOB_ID</span><span class=si>}</span><span class=s2>;
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;</span>
</span></span></code></pre></div><h2 id=5-port-forwarding>5. Port Forwarding</h2><p>If you&rsquo;re running training jobs on the burst cluster, there&rsquo;s a high chance you might want to use tensorboard as well. To access tensorboard on your local system, you need multiple levels of port forwarding. This is because the file system on the burst compute nodes is only accessible to the compute nodes, which in turn are accessible from the burst login node, which in turn is accessible from greene, which in turn is accessible from the NYU gateway. Yes, that&rsquo;s quite a lot of layers.</p><p>First, you need to spin up a compute node on the burst cluster and run tensorboard on the node.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Run this on a burst compute node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Let&#39;s say the hostname is b-9-9</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> tensorboard --logdir runs
</span></span></code></pre></div><p>Next, open up a local terminal and set up a series of port forwards. Note that in the following snippet I&rsquo;m using the SSH alias for <code>greene</code> as described in <a href=#first-things-first>Section 1</a> of this tutorial.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> ssh -t -L 6006:localhost:6006 greene <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=gp>$</span>   ssh -t -L 6006:localhost:6006 burst <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=gp>$</span>     ssh -t -L 6006:localhost:6006 b-9-9 
</span></span></code></pre></div><p>Without SSH aliases, the forwarding chain will look something like this.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> ssh -t -L 6006:localhost:6006 abc1234@greene.hpc.nyu.edu <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=gp>$</span>   ssh -t -L 6006:localhost:6006 burst <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=gp>$</span>     ssh -t -L 6006:localhost:6006 b-9-9
</span></span></code></pre></div><h1 id=troubleshooting>Troubleshooting</h1><hr><h2 id=1-singularity-cant-open-overlay-for-writing>1. Singularity can&rsquo;t open overlay for writing</h2><p>If you were actively using a Singularity with an ext3 overlay, and for some reason you didn&rsquo;t exit cleanly (maybe slurm preempted your job), you may run into issues. The next time you try using Singularity, you might get this error complaining that your overlay file is still in use:</p><pre tabindex=0><code>FATAL:
while loading overlay images: failed to open overlay image /path/to/overlay.ext3: 
    while locking ext3 partition from /path/to/overlay.ext3: 
        can&#39;t open //path/to/overlay.ext3 for writing, currently in use by another process
</code></pre><p>First, I&rsquo;d check if what the error says is true.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> ps aux <span class=p>|</span> grep Singularity
</span></span></code></pre></div><p>If there&rsquo;s an active process, use <code>kill</code> to terminate it. If there&rsquo;s no active process and you&rsquo;re sure that your overlay file is clean to use, you can run <code>fsck</code> and clean the dirty bit. For more info see <a href=https://github.com/apptainer/singularity/issues/6196>this github issue</a> on the singularity repo.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> fsck.ext3 /path/to/overlay.ext3
</span></span></code></pre></div><h2 id=2-job-killed-due-of-low-gpu-usage>2. Job killed due of low GPU usage</h2><p>You should only request a GPU when you need it. However, if you need a dummy script to keep the GPU busy (which I highly recommend against), have a look at this <a href=https://gist.github.com/nikhilweee/b5a2a201f97c386f4701d48cbf7f5a04>gist</a>.</p><p>Hope this helps!</p></div></article><!-- end: layouts/_default/single.html --></div><!-- begin: layouts/partials/footer.html --><div class=wrapper><div class=giscus></div></div><footer class=site-footer><div class=wrapper><p>&copy; 2022 <a href=/>Nikhil Verma</a> • Subscribe via <a href=/feed.xml>RSS</a></p></div></footer><!-- end: layouts/partials/footer.html --></main><div class="sidebar sidebar-right"><aside class=sidebar-toc><h1>Outline</h1><nav id=TableOfContents><ul><li><a href=#getting-started>Getting Started</a><ul><li><a href=#1-passwordless-ssh>1. Passwordless SSH</a></li><li><a href=#2-set-up-ssh-aliases>2. Set up SSH aliases</a></li></ul></li><li><a href=#login-hacks>Login Hacks</a><ul><li><a href=#1-login-directly-into-compute-nodes>1. Login directly into compute nodes</a></li><li><a href=#2-use-local-ssh-keys>2. Use local SSH keys</a></li></ul></li><li><a href=#some-slurm-aliases>Some Slurm Aliases</a><ul><li><a href=#1-common-aliases>1. Common aliases</a></li><li><a href=#2-view-your-job-queue>2. View your job queue</a></li><li><a href=#3-view-output-from-your-job>3. View output from your job</a></li></ul></li><li><a href=#some-hidden-gems>Some Hidden Gems</a></li><li><a href=#burst-usage>Burst Usage</a><ul><li><a href=#1-housekeeping>1. Housekeeping</a></li><li><a href=#2-compute-nodes>2. Compute Nodes</a></li><li><a href=#3-data-transfer>3. Data Transfer</a></li><li><a href=#4-gpu-nodes>4. GPU Nodes</a></li><li><a href=#5-port-forwarding>5. Port Forwarding</a></li></ul></li><li><a href=#troubleshooting>Troubleshooting</a><ul><li><a href=#1-singularity-cant-open-overlay-for-writing>1. Singularity can&rsquo;t open overlay for writing</a></li><li><a href=#2-job-killed-due-of-low-gpu-usage>2. Job killed due of low GPU usage</a></li></ul></li></ul></nav></aside></div></body><script type=text/javascript>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light"),changeGiscusTheme()):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"),changeGiscusTheme())});function changeGiscusTheme(){let t=localStorage.getItem("pref-theme"),n={setConfig:{theme:t}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:n},"https://giscus.app")}(localStorage.getItem("pref-theme")==="dark"||window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.getElementById("theme-toggle").click();function handleMessage(e){if(e.origin!=="https://giscus.app")return;if(typeof e.data!="object"||!e.data.giscus)return;const t=e.data.giscus;changeGiscusTheme()}window.addEventListener("message",handleMessage)</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-0JVB0SEFDJ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0JVB0SEFDJ",{anonymize_ip:!1})}</script><script src=https://giscus.app/client.js data-repo=nikhilweee/nikhilweee.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkzOTU5NjUwNTc=" data-category=Comments data-category-id=DIC_kwDOF5nygc4COpdc data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script><!-- end: layouts/_default/baseof.html --></html>