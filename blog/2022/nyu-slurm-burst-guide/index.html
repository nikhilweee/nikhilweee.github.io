<!doctype html><!-- begin: layouts/_default/baseof.html --><html lang=en><!-- begin: layouts/partials/head.html --><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.111.3"><title>Cloud Bursting with Slurm - weeeblog</title><link rel=icon type=image/svg href=/static/favicon.png><link rel=stylesheet type=text/css href=/main.css><link rel=canonical href=https://nikhilweee.me/blog/2022/nyu-slurm-burst-guide/><link href=https://cdn.jsdelivr.net/npm/charter-webfont/charter.min.css rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css></head><!-- end: layouts/partials/head.html --><body class=flex-parent><div class="sidebar sidebar-left"></div><main class=flex-content><!-- begin: layouts/partials/header.html --><div class=wrapper><header class=site-header><nav class=site-title><a href=/blog><strong>weeeblog</strong></a></nav><nav class=site-nav><a href=/>About</a>
<a href=/blog/>Blog</a>
<a href=/blog/archives>Archives</a></nav><nav class=site-toggle><a id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></nav></header></div><!-- end: layouts/partials/header.html --><div class="page-content wrapper"><!-- begin: layouts/_default/single.html --><article class=post itemscope itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class=post-title itemprop="name headline">Cloud Bursting with Slurm</h1><p class=post-subtitle itemprop="abstract description">Using burst nodes on NYU's HPC cluster</p><p class=post-meta><span itemprop=author itemscope itemtype=http://schema.org/Person><a href=/><span itemprop=name>Nikhil Verma</span></a></span> •
<time datetime="2022-11-11 00:00:00 +0000 UTC" itemprop=datePublished>Published Nov 11, 2022</time> •
<a href=/blog/archives/references/>References</a></p></header><div class=post-content itemprop=articleBody><p>Here&rsquo;s a guide on how to access NYU&rsquo;s burst cluster for graduate courses. I created this guide for the Fall 2022 edition of the Computer Vision course, but a lot of this should still be applicable for other courses. Please feel free to adapt this to your use case. Further readings can be found towards the end of this post.</p><h1 id=cluster-configuration>Cluster Configuration</h1><hr><p>Different nodes in the NYU cluster are connected in the following manner. You will access everything through SSH. This schematic should be helpful in understanding the rest of this guide.</p><p>The Greene login node will be your gateway to NYU&rsquo;s on-premise cluster. For this course, you will be running jobs on one of the burst compute nodes, which you will access through the burst login node.</p><pre tabindex=0><code>                                            +--------------------+      +--------------------+ 
                                            |                    |      |                    | 
                                            |                    |      |                    | 
                                            |   Greene Compute   |      |   Burst Compute    | 
                                            |                    |      |                    | 
                                            |                    |      |                    | 
                                            +----------|---------+      +----------|---------+ 
                                                       |                           |           
                                                       |                           |           
        +-----------------+                 +----------|---------+      +----------|---------+ 
        |                 |                 |                    |      |                    | 
        |                 |                 |                    |      |                    | 
        |      Local      &lt;=== (NYU VPN) ===&gt;    Greene Login    --------    Burst Login     | 
        |                 |                 |                    |      |                    | 
        |                 |                 |                    |      |                    | 
        +-----------------+                 +--------------------+      +--------------------+ 
</code></pre><h1 id=burst-setup>Burst Setup</h1><hr><p><strong>1. Check Acess to Greene</strong>: First off, you should check whether you have access to Greene, which is NYU&rsquo;s compute cluster. If you&rsquo;re connecting from home, please make sure you&rsquo;re using NYU&rsquo;s VPN.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On your local machine</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh &lt;netid&gt;@greene.hpc.nyu.edu
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should be logged into a greene login node</span>
</span></span></code></pre></div><p>The Greene cluster and its on-premise resources are primarily meant for researchers. For compute requirements during courses like this one, NYU has a separate <code>burst</code> setup. This allows the cluster to offload additional usage to a third party cloud provider. In our case, usage is handled by Google Cloud.</p><p><strong>2. Login to Burst Node</strong>: To use the burst setup, you should first login to the <code>burst</code> node from <code>greene</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a greene login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh burst
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should be logged into a burst login node</span>
</span></span></code></pre></div><p><strong>3. Check Bursting Setup</strong>: Next, check whether you are associated with the burst account for this class.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sacctmgr show assoc <span class=nv>user</span><span class=o>=</span><span class=nv>$USER</span> <span class=nv>format</span><span class=o>=</span>Account%30,GrpTRESMins%30
</span></span><span class=line><span class=cl><span class=go>                       Account                    GrpTRESMins 
</span></span></span><span class=line><span class=cl><span class=go>------------------------------ ------------------------------ 
</span></span></span><span class=line><span class=cl><span class=go>       csci_ga_2271_001-2022fa      cpu=120000,gres/gpu=12000 
</span></span></span></code></pre></div><p>If you&rsquo;ve been assigned an account for the CV class, you should see the row <code>csci_ga_2271_001-2022fa</code>. The output suggests that you should have access to 2000 hours of CPU usage and 200 hours of GPU usage.</p><p><strong>4. Check Resource Usage</strong>: You should periodically keep track of your usage.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sshare --user<span class=o>=</span><span class=nv>$USER</span>  --format<span class=o>=</span>Account%30,GrpTRESRaw%90
</span></span><span class=line><span class=cl><span class=go>                       Account                                                                                 GrpTRESRaw 
</span></span></span><span class=line><span class=cl><span class=go>------------------------------ ------------------------------------------------------------------------------------------ 
</span></span></span><span class=line><span class=cl><span class=go>       csci_ga_2271_001-2022fa        cpu=112,mem=415950,energy=0,node=7,billing=112,fs/disk=0,vmem=0,pages=0,gres/gpu=14 
</span></span></span></code></pre></div><p>This output represents 112 minutes of CPU usage (<code>cpu=112</code>) and 14 minutes of GPU usage (<code>gres/gpu=14</code>). Be extra careful about GPU usage as it can add up quickly. Keep in mind that these are cumulative quotas so running a job on 4 GPUs for 30 minutes will deplete your available GPU quota by 120 minutes, not 30.</p><h1 id=data-transfer>Data Transfer</h1><hr><p>Something worth knowing is that the greene login node, the greene compute nodes, and the burst login node share the same filesystem. However, you will run jobs on one of the burst compute nodes, which have a separate filesystem. To transfer data from greene to one of the burst compute nodes, use <code>rsync</code> to copy data from <code>greene-dtn</code>. This is a special node on the greene cluster optimized for data transfer.</p><p><strong>5. Data Transfer</strong>: Login to a burst compute node through an interactive job, then transfer data using <code>rsync</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> srun --account<span class=o>=</span>csci_ga_2271_001-2022fa --partition<span class=o>=</span>interactive --pty /bin/bash
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should be logged into a burst compute node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> rsync -aP &lt;netid&gt;@greene-dtn.hpc.nyu.edu:/source/file/path/on/greene /destination/file/path/on/burst
</span></span></code></pre></div><p>All burst compute nodes share the same filesystem, so you should only have to do this once.</p><h1 id=partitions>Partitions</h1><hr><p>As you might have noticed so far, while submitting a job, you also need to specify a partition. Each partition gives you access to different kinds of resources. For the course, you have access to the following partitions. The resources offered by each partition are also listed in the table.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> sinfo -O <span class=s2>&#34;Partition,CPUs,Memory,GRes&#34;</span>
</span></span><span class=line><span class=cl><span class=go>PARTITION           CPUS                MEMORY (MB)         GPUS        
</span></span></span><span class=line><span class=cl><span class=go>interactive         2                   1500                (null)      
</span></span></span><span class=line><span class=cl><span class=go>c2s16p              16                  64000               (null)      
</span></span></span><span class=line><span class=cl><span class=go>n1s8-v100-1         8                   29000               gpu:v100:1  
</span></span></span><span class=line><span class=cl><span class=go>n1s16-v100-2        16                  59000               gpu:v100:2  
</span></span></span><span class=line><span class=cl><span class=go>n1c10m64-v100-1     10                  64000               gpu:v100:1  
</span></span></span><span class=line><span class=cl><span class=go>n1c16m96-v100-2     16                  96000               gpu:v100:2  
</span></span></span></code></pre></div><p>Note that each V100 GPU has 16GB of memory. Partitions with <code>gpu:v100:1</code> have one V100 GPU available for every job. Partitions with <code>gpu:v100:2</code> have two V100 GPUs available for each of your jobs, and so on.</p><h1 id=submitting-jobs>Submitting Jobs</h1><hr><p>You have the option to either submit an interactive job, or submit a batch job. Jobs can take anywhere from a few seconds to a few minutes of provisioning time to become fully available. Under the hood, a new VM will be allocated for you on Google Cloud, so this takes some time. Please be patient.</p><h2 id=interactive-jobs>Interactive Jobs</h2><p>An interactive job is straightforward to run, but will terminate as soon as you lose network connection or exit the terminal. You can launch an interactive job by using the <code>srun</code> command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> srun --account<span class=o>=</span>csci_ga_2271_001-2022fa --partition<span class=o>=</span>n1s8-v100-1 --gres<span class=o>=</span>gpu:1 --pty /bin/bash
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># Wait for a few seconds to a few minutes</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should be logged into a burst compute node</span>
</span></span></code></pre></div><p>Note that there&rsquo;s an additional argument <code>--gres=gpu:1</code> passed because we&rsquo;re using a partition with a single GPU. Once the job is allocated, you can run your training scripts as you would normally do.</p><h2 id=batch-jobs>Batch Jobs</h2><p>A batch job will not terminate before its time limit, unless explicitly killed. However, there are some extra steps involved. You can launch a batch job using the <code>sbatch</code> command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> sbatch --account<span class=o>=</span>csci_ga_2271_001-2022fa --partition<span class=o>=</span>n1s16-v100-2 --gres<span class=o>=</span>gpu:2 --time<span class=o>=</span>01:00:00 --wrap <span class=s2>&#34;sleep infinity&#34;</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should immediately get back the job ID</span>
</span></span><span class=line><span class=cl><span class=go>Submitted batch job 88344
</span></span></span><span class=line><span class=cl><span class=go></span><span class=gp>$</span> <span class=c1># Note that you&#39;re still on the burst login node</span>
</span></span></code></pre></div><p>Again, note that there&rsquo;s an additional argument <code>--gres=gpu:2</code> passed because we&rsquo;re using a partition with two GPUs. To access the compute node, you need to find out the hostname of the node being provisioned.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> squeue -u <span class=nv>$USER</span> -O <span class=s2>&#34;JobID:10,Partition:15,Nodelist:10,State:12,StartTime:21,TimeLeft:11,Account:25,Name:40&#34;</span>
</span></span><span class=line><span class=cl><span class=go>JOBID     PARTITION      NODELIST  STATE       START_TIME           TIME_LEFT  ACCOUNT                  NAME                                    
</span></span></span><span class=line><span class=cl><span class=go>88345     n1s16-v100-2   b-6-1     RUNNING     2022-09-08T02:25:03  59:45      csci_ga_2271_001-2022fa  wrap                                    
</span></span></span></code></pre></div><p>The hostname of the compute node is under the header <code>NODELIST</code>. In this case, it is <code>b-6-1</code>. You can login to the compute node simply by using <code>ssh</code>. Please keep an eye on the <code>STATE</code> of the job. You will only be able to ssh into compute nodes which are in the <code>RUNNING</code> state.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst login node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> ssh b-6-1
</span></span><span class=line><span class=cl><span class=gp>$</span> <span class=c1># You should be logged into a burst compute node</span>
</span></span></code></pre></div><p>To cancel a job before it expires, use the <code>scancel</code> command. You will need to specify a JOBID.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> <span class=c1># On a burst node</span>
</span></span><span class=line><span class=cl><span class=gp>$</span> scancel <span class=m>88345</span>
</span></span></code></pre></div><p>The running job should now be cancelled.</p><h1 id=advanced-usage>Advanced Usage</h1><hr><p>If you don&rsquo;t wish to type out the long commands, you can use a bash script with SBATCH directives:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --time=12:00:00</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --gres=gpu:1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --output=&#34;%A_%x.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --account=csci_ga_2271_001-2022fa</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --partition=n1s8-v100-1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --job-name=train_both</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>singularity <span class=nb>exec</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --nv --overlay /scratch/abc1234/overlay-50G-10M.ext3:ro <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    /scratch/abc1234/cuda11.3.0-cudnn8-devel-ubuntu20.04.sif <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    /bin/bash -c <span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    source /ext3/miniconda3/etc/profile.d/conda.sh;
</span></span></span><span class=line><span class=cl><span class=s2>    conda activate env;
</span></span></span><span class=line><span class=cl><span class=s2>    cd /home/abc1234/projects/cv;
</span></span></span><span class=line><span class=cl><span class=s2>    python -u train.py --batch_size 40 \
</span></span></span><span class=line><span class=cl><span class=s2>        --output_dir runs/</span><span class=si>${</span><span class=nv>SLURM_JOB_NAME</span><span class=si>}</span><span class=s2>_</span><span class=si>${</span><span class=nv>SLURM_JOB_ID</span><span class=si>}</span><span class=s2>;
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;</span>
</span></span></code></pre></div><p>This script also makes use of Singularity containers. More info can be found in the next section.</p><h1 id=further-resources>Further Resources</h1><hr><p>For more information, feel free to check out the relevant documentation from NYU HPC.</p><ul><li>Submitting Jobs: <a href=https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/slurm-submitting-jobs>https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/slurm-submitting-jobs</a></li><li>Singluarity: <a href=https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/singularity-run-custom-applications-with-containers>https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/singularity-run-custom-applications-with-containers</a></li><li>SLURM Commands: <a href=https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/slurm-main-commands>https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/slurm-main-commands</a></li><li>Crowdsourced Cluster Support: <a href=https://github.com/nyu-dl/cluster-support>https://github.com/nyu-dl/cluster-support</a></li></ul></div></article><!-- end: layouts/_default/single.html --></div><!-- begin: layouts/partials/footer.html --><div class=wrapper><div class=giscus></div></div><footer class=site-footer><div class=wrapper><p>&copy; 2023 <a href=/>Nikhil Verma</a><br><a href=/feed.xml>RSS</a> • <a href=https://github.com/nikhilweee/hugo-whiteglass>Theme</a></p></div></footer><!-- end: layouts/partials/footer.html --></main><div class="sidebar sidebar-right"><aside class=sidebar-toc><h1>Outline</h1><nav id=TableOfContents><ul><li><a href=#cluster-configuration>Cluster Configuration</a></li><li><a href=#burst-setup>Burst Setup</a></li><li><a href=#data-transfer>Data Transfer</a></li><li><a href=#partitions>Partitions</a></li><li><a href=#submitting-jobs>Submitting Jobs</a><ul><li><a href=#interactive-jobs>Interactive Jobs</a></li><li><a href=#batch-jobs>Batch Jobs</a></li></ul></li><li><a href=#advanced-usage>Advanced Usage</a></li><li><a href=#further-resources>Further Resources</a></li></ul></nav></aside></div></body><script type=text/javascript>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light"),changeGiscusTheme()):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"),changeGiscusTheme())});function changeGiscusTheme(){let t=localStorage.getItem("pref-theme"),n={setConfig:{theme:t}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:n},"https://giscus.app")}localStorage.getItem("pref-theme")!="light"&&window.matchMedia("(prefers-color-scheme: dark)").matches&&document.getElementById("theme-toggle").click();function handleMessage(e){if(e.origin!=="https://giscus.app")return;if(typeof e.data!="object"||!e.data.giscus)return;const t=e.data.giscus;changeGiscusTheme()}window.addEventListener("message",handleMessage)</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-0JVB0SEFDJ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0JVB0SEFDJ",{anonymize_ip:!1})}</script><script src=https://giscus.app/client.js data-repo=nikhilweee/nikhilweee.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkzOTU5NjUwNTc=" data-category=Comments data-category-id=DIC_kwDOF5nygc4COpdc data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script><!-- end: layouts/_default/baseof.html --></html>