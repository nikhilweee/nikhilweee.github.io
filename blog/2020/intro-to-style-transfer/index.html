<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Intro to Style Transfer - weeeblog</title>
<meta name=description content="Image Style Transfer Using Convolutional Neural Networks This paper introduced neural style transfer. It uses a VGGNet pre-trained on the ImageNet dataset for the purpose. The key idea is to be able to separate content and style from the representations of the network. Once that is done, a new image is synthesized from white noise where two different kind of losses are minimized - a style loss between the style image and the hybrid image, and a content loss between the content image and the hybrid image."><link rel=icon type=image/svg href=/static/favicon.png>
<link rel=stylesheet href=/main.css>
<link rel=canonical href=https://nikhilweee.me/blog/2020/intro-to-style-transfer/>
<script type=text/x-mathjax-config>
            MathJax.Hub.Config({
                tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
                CommonHTML: { linebreaks: { automatic: true } },
            });
        </script>
<script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href=https://cdn.jsdelivr.net/npm/charter-webfont/charter.min.css rel=stylesheet>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css>
</head>
<body>
<header class=site-header>
<div class=wrapper>
<a class=site-title href=/blog>weeeblog</a>
<nav class=site-nav>
<a class=page-link href=/blog/archives/>Archives</a>
<a class=page-link href=/>About</a>
</nav>
</div>
</header>
<main class=page-content>
<div class=wrapper>
<article class=post itemscope itemtype=http://schema.org/BlogPosting>
<header class=post-header>
<h1 class=post-title itemprop="name headline">Intro to Style Transfer</h1>
<h2 class=post-subtitle itemprop="abstract description">Image Style Transfer Using Convolutional Neural Networks</h2>
<p class=post-meta>
<time datetime="2020-09-11 00:00:00 +0000 UTC" itemprop=datePublished>Sep 11, 2020</time>
• <span itemprop=author itemscope itemtype=http://schema.org/Person>
<a href=/><span itemprop=name>Nikhil Verma</span></a></span>
•
<a href=/blog/categories/papers/>Papers</a>
</p>
</header>
<div class=post-content itemprop=articleBody>
<h2 id=image-style-transfer-using-convolutional-neural-networks>Image Style Transfer Using Convolutional Neural Networks</h2>
<p>This paper introduced neural style transfer. It uses a VGGNet pre-trained on the ImageNet dataset for the purpose. The key idea is to be able to separate content and style from the representations of the network. Once that is done, a new image is synthesized from white noise where two different kind of losses are minimized - a style loss between the style image and the hybrid image, and a content loss between the content image and the hybrid image.</p>
<h2 id=intuitions>Intuitions</h2>
<p>Since the VGGNet was originally trained for object classification, the layers towards the end of the network should have enough information to be able to recognise the object while still being invariant to its lower level features like position, style, etc. Therefore, we use the higher layers for the purposes of extracting content from the image, and the lower layers for the purposes of capturing the style of the image. This intuition is formalised in the way the losses are calculated.</p>
<h2 id=content-loss>Content Loss</h2>
<p>The representation of every image $\vec{x}$ in each layer $l$ of a CNN can be encoded by the feature response $F^l$ to the layer. Therefore, for the hybrid image $\vec{x}$ to be able to match the content of the original image $\vec{p}$, their respective feature representations $F^l$ and $P^l$ should be similar. This gives rise to the content loss, which is simply the squared error loss between the two. Note that $F^l \in \mathbb{R}^{N_l \times M_l}$ where $F_{ij}^{l}$ is the activation of the $i$th filter at the $j$th position in layer $l$ with $N_l$ distinct features each of size $M_l$</p>
<p>$$ L_{content} (\vec{p}, \vec{x}, l) = \frac{1}{2} \sum_{i, j} (F_{ij}^l - P_{ij}^l) $$</p>
<h2 id=style-loss>Style Loss</h2>
<p>To capture the style of an image, we might just want to capture the feature responses from the lower layers of the image. But note that those layers also contain spatial information about the content of the image which are later used by the higher layers of the network. Therefore, there is a need to decouple that information from the style of the image. To do so, we use a matrix of feature correlations built on top of the feature responses of each layer in the CNN. The feature correlations are given by something called as the Gram matrix $G^l \in \mathbb{R}^{N_l \times N_l}$, where $G_{ij}^l$ is the inner product between the feature maps $i$ and $j$ in layer $l$.</p>
<p>$$
G_{ij}^l = \sum_k F_{ik}^l F_{jk}^l
$$</p>
<p>If $\vec{a}$ and $\vec{x}$ are the style image and the hybrid image respectively, then just like the content loss, we want the gram matrix of the style image $A^l$ to be as close as possible to the gram matrix of the hybrid image $G^l$. The style loss $E_l$ for every layer $l$ of the network is then defined as the squared loss between the two gram matrices. The total style loss is the weighted sum of the individual layer losses $E_l$ where $w_l$ are the weighing factors described separately in the paper.</p>
<p>$$
L_{style} (\vec{a}, \vec{x}) = \sum_{l=0}^{L} w_l E_l
\hspace{2cm}
E_l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} (G_{ij}^l - A_{ij}^l)^2
$$</p>
<p>The total loss $L_{total}$ is the weighted sum of the content and style losses with weights $\alpha$ and $\beta$ respectively.</p>
<p>$$
L_{total} (\vec{p}, \vec{a}, \vec{x}) = \alpha L_{content} (\vec{p}, \vec{x}) + \beta L_{style} (\vec{a}, \vec{x})
$$</p>
</div>
<div id=disqus_thread></div>
<script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//nikhilweee.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script>
<noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript>
<a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a>
</article>
</div>
</main>
<footer class=site-footer>
<div class=wrapper>
<p>
&copy; 2022 Nikhil Verma • Subscribe via <a href=/feed.xml>RSS</a>
</p>
</div>
</footer>
</body>
</html>